{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0dcf50-8ddd-4512-9eed-ece051f11c4a",
   "metadata": {},
   "source": [
    "## Creating an image decoder (Classifying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60011ab-f4fe-44f3-bdf8-1cc7e10ee5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceba79b7-89ef-4f44-9f49-77facd2a8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d383585a-d0d0-473e-9d02-8469c0d043cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "folder_path = 'video_frames/'\n",
    "\n",
    "image_paths = os.listdir(folder_path)\n",
    "image_paths = [folder_path + img_path for img_path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3464c74f-7451-471a-aa23-1bf28369d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = []\n",
    "for img in image_paths:\n",
    "    y.append( cv2.imread(img, cv2.IMREAD_GRAYSCALE) )\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.arange(y.shape[0])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "#X = nn.functional.one_hot(X, num_classes=y.shape[0]).float()\n",
    "y = torch.tensor(y).float()\n",
    "y = y/255 # ensures that values scale between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16aa2f43-4fb5-47ad-b8c0-f997b73d58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 6572\n",
      "Output dimensions: torch.Size([180, 240])\n"
     ]
    }
   ],
   "source": [
    "num_classes = y.shape[0]\n",
    "output_dimensions = y[0].shape\n",
    "print(f\"Num classes: {num_classes}\")\n",
    "print(f\"Output dimensions: {output_dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62202ba-d225-427e-8019-f7b7c71ae760",
   "metadata": {},
   "source": [
    "### Fully connected NN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4fcef8-6e60-4073-b134-af3b335f9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Conv_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # FC layer for feature mapping\n",
    "        self.fc = nn.Linear(num_classes, latent_dim * 3 * 4)\n",
    "\n",
    "        # Deconvolution layers\n",
    "        self.conv_1 = nn.ConvTranspose2d(latent_dim, 10,\n",
    "                           kernel_size=5, stride=5, padding=0)\n",
    "        self.conv_2 = nn.ConvTranspose2d(10, 10, \n",
    "                           kernel_size=3, stride=3, padding=0)\n",
    "        self.conv_3 = nn.ConvTranspose2d(10, 10,\n",
    "                           kernel_size=2, stride=2, padding=0)\n",
    "        self.conv_4 = nn.ConvTranspose2d(10, num_classes,\n",
    "                           kernel_size=2, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.one_hot(x, num_classes=self.num_classes).float()\n",
    "        x = self.fc(x)\n",
    "        x = x.view(self.latent_dim, 3, 4)\n",
    "        print(f\"Shape after conv0: {x.shape}\")\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Shape after conv1: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Shape after conv2: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Shape after conv3: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        print(f\"Shape after conv4: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a55bbf-889e-4a07-bf0e-cb6625f2a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # FC layer for feature mapping\n",
    "        self.fc = nn.Linear(num_classes, latent_dim * 3 * 4)\n",
    "\n",
    "        # Deconvolution layers\n",
    "        self.deconv_block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 100, \n",
    "                               kernel_size=5, stride=5, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(100, 100, \n",
    "                               kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(100, 100,\n",
    "                               kernel_size=2, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(100, 1,\n",
    "                               kernel_size=2, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.one_hot(x, num_classes=self.num_classes).float()\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.latent_dim, 3, 4)\n",
    "        x = self.deconv_block(x)\n",
    "        x = x.view(-1, 180, 240)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366c1336-07e7-4c46-a043-833e7774a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X1 = nn.functional.one_hot(X[5], num_classes=num_classes).float()\n",
    "linear_ = nn.Linear(num_classes, latent_dim * 3 * 4)\n",
    "X2 = linear_(X1)\n",
    "X3 = X2.view(-1, latent_dim, 3, 4)\n",
    "X3.shape\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6594f2f-4e41-44cb-9819-5a13d21931ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_Decoder(\n",
      "  (fc): Linear(in_features=6572, out_features=12, bias=True)\n",
      "  (deconv_block): Sequential(\n",
      "    (0): ConvTranspose2d(1, 100, kernel_size=(5, 5), stride=(5, 5))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(100, 100, kernel_size=(3, 3), stride=(3, 3))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(100, 100, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(100, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Conv_Decoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65c31211-1f6d-4cbf-b6a1-70ff9bddd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "decay_1 = 0.9 # Decay of moving average of gradient\n",
    "decay_2 = 0.99 # Decay of moving average of squared gradient\n",
    "\n",
    "lr = 0.00001\n",
    "weight_decay = 0.0000000004\n",
    "\n",
    "lr_decay_rate = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db1ae32d-dcf0-43b0-91ac-34042f72658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, Optimizer and Scheduler\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              betas = (decay_1, decay_2),\n",
    "                              lr=lr, \n",
    "                              weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da3dd411-d6ab-49d4-a62f-454da8b4427a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96694f66-e22f-43e4-93ff-5e1f1110db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "inputs_device = X.device\n",
    "labels_device = y.device\n",
    "\n",
    "print(model_device)\n",
    "print(inputs_device)\n",
    "print(labels_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0217652b-d970-4482-93bd-52ef5fd9055b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 240])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a06b5b2e-bc3f-4889-9d60-334419b2e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 240])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(0, dtype=torch.long).to(device))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4896c69e-1108-4355-8cb7-d8e1c6cdb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(TensorDataset(X, y), batch_size=200, shuffle=True) # both train and test (deliberate overfit)\n",
    "\n",
    "num_epochs = 100\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61eda7b4-298b-4b4a-a176-048147ce6964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.044592\n",
      "Epoch 2/100, Loss: 0.044397\n",
      "Epoch 3/100, Loss: 0.044275\n",
      "Epoch 4/100, Loss: 0.044185\n",
      "Epoch 5/100, Loss: 0.044108\n",
      "Epoch 6/100, Loss: 0.044031\n",
      "Epoch 7/100, Loss: 0.043989\n",
      "Epoch 8/100, Loss: 0.043951\n",
      "Epoch 9/100, Loss: 0.043918\n",
      "Epoch 10/100, Loss: 0.043883\n",
      "Epoch 11/100, Loss: 0.043871\n",
      "Epoch 12/100, Loss: 0.043850\n",
      "Epoch 13/100, Loss: 0.043835\n",
      "Epoch 14/100, Loss: 0.043802\n",
      "Epoch 15/100, Loss: 0.043793\n",
      "Epoch 16/100, Loss: 0.043801\n",
      "Epoch 17/100, Loss: 0.043773\n",
      "Epoch 18/100, Loss: 0.043769\n",
      "Epoch 19/100, Loss: 0.043768\n",
      "Epoch 20/100, Loss: 0.043768\n",
      "Epoch 21/100, Loss: 0.043759\n",
      "Epoch 22/100, Loss: 0.043739\n",
      "Epoch 23/100, Loss: 0.043745\n",
      "Epoch 24/100, Loss: 0.043758\n",
      "Epoch 25/100, Loss: 0.043730\n",
      "Epoch 26/100, Loss: 0.043734\n",
      "Epoch 27/100, Loss: 0.043718\n",
      "Epoch 28/100, Loss: 0.043729\n",
      "Epoch 29/100, Loss: 0.043704\n",
      "Epoch 30/100, Loss: 0.043714\n",
      "Epoch 31/100, Loss: 0.043722\n",
      "Epoch 32/100, Loss: 0.043718\n",
      "Epoch 33/100, Loss: 0.043708\n",
      "Epoch 34/100, Loss: 0.043716\n",
      "Epoch 35/100, Loss: 0.043703\n",
      "Epoch 36/100, Loss: 0.043692\n",
      "Epoch 37/100, Loss: 0.043694\n",
      "Epoch 38/100, Loss: 0.043700\n",
      "Epoch 39/100, Loss: 0.043699\n",
      "Epoch 40/100, Loss: 0.043683\n",
      "Epoch 41/100, Loss: 0.043689\n",
      "Epoch 42/100, Loss: 0.043678\n",
      "Epoch 43/100, Loss: 0.043693\n",
      "Epoch 44/100, Loss: 0.043663\n",
      "Epoch 45/100, Loss: 0.043679\n",
      "Epoch 46/100, Loss: 0.043664\n",
      "Epoch 47/100, Loss: 0.043690\n",
      "Epoch 48/100, Loss: 0.043698\n",
      "Epoch 49/100, Loss: 0.043672\n",
      "Epoch 50/100, Loss: 0.043680\n",
      "Epoch 51/100, Loss: 0.043673\n",
      "Epoch 52/100, Loss: 0.043672\n",
      "Epoch 53/100, Loss: 0.043665\n",
      "Epoch 54/100, Loss: 0.043671\n",
      "Epoch 55/100, Loss: 0.043666\n",
      "Epoch 56/100, Loss: 0.043676\n",
      "Epoch 57/100, Loss: 0.043651\n",
      "Epoch 58/100, Loss: 0.043658\n",
      "Epoch 59/100, Loss: 0.043652\n",
      "Epoch 60/100, Loss: 0.043644\n",
      "Epoch 61/100, Loss: 0.043645\n",
      "Epoch 62/100, Loss: 0.043644\n",
      "Epoch 63/100, Loss: 0.043661\n",
      "Epoch 64/100, Loss: 0.043651\n",
      "Epoch 65/100, Loss: 0.043646\n",
      "Epoch 66/100, Loss: 0.043647\n",
      "Epoch 67/100, Loss: 0.043650\n",
      "Epoch 68/100, Loss: 0.043649\n",
      "Epoch 69/100, Loss: 0.043663\n",
      "Epoch 70/100, Loss: 0.043643\n",
      "Epoch 71/100, Loss: 0.043653\n",
      "Epoch 72/100, Loss: 0.043641\n",
      "Epoch 73/100, Loss: 0.043651\n",
      "Epoch 74/100, Loss: 0.043634\n",
      "Epoch 75/100, Loss: 0.043654\n",
      "Epoch 76/100, Loss: 0.043639\n",
      "Epoch 77/100, Loss: 0.043633\n",
      "Epoch 78/100, Loss: 0.043650\n",
      "Epoch 79/100, Loss: 0.043629\n",
      "Epoch 80/100, Loss: 0.043630\n",
      "Epoch 81/100, Loss: 0.043639\n",
      "Epoch 82/100, Loss: 0.043632\n",
      "Epoch 83/100, Loss: 0.043642\n",
      "Epoch 84/100, Loss: 0.043629\n",
      "Epoch 85/100, Loss: 0.043637\n",
      "Epoch 86/100, Loss: 0.043649\n",
      "Epoch 87/100, Loss: 0.043629\n",
      "Epoch 88/100, Loss: 0.043618\n",
      "Epoch 89/100, Loss: 0.043636\n",
      "Epoch 90/100, Loss: 0.043631\n",
      "Epoch 91/100, Loss: 0.043635\n",
      "Epoch 92/100, Loss: 0.043629\n",
      "Epoch 93/100, Loss: 0.043621\n",
      "Epoch 94/100, Loss: 0.043630\n",
      "Epoch 95/100, Loss: 0.043632\n",
      "Epoch 96/100, Loss: 0.043622\n",
      "Epoch 97/100, Loss: 0.043629\n",
      "Epoch 98/100, Loss: 0.043613\n",
      "Epoch 99/100, Loss: 0.043607\n",
      "Epoch 100/100, Loss: 0.043614\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "loss_values = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train() # Set to train mode\n",
    "    for inputs, labels in dataloader:\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed-forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add loss\n",
    "        batch_loss = loss.item()\n",
    "        running_loss += batch_loss\n",
    "        loss_values.append(batch_loss)\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # Update the learning rate at the end of each epoch\n",
    "    scheduler.step()\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d1bbb-a7eb-420d-b0ba-b3bceba60a18",
   "metadata": {},
   "source": [
    "### Model is too small to fully learn the images. I will freeze current weights and add another layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d69b5a6-9470-4229-8c91-84542cc1a891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv_Decoder(\n",
       "  (fc): Linear(in_features=6572, out_features=12, bias=True)\n",
       "  (deconv_block): Sequential(\n",
       "    (0): ConvTranspose2d(1, 100, kernel_size=(5, 5), stride=(5, 5))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(100, 100, kernel_size=(3, 3), stride=(3, 3))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(100, 100, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(100, 1, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78dcab43-7136-4cc9-8682-85faaac32f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000237829F5A80>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freezing params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600e797-223f-4bfa-811d-b1875abea1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bae1f1-ab05-491a-8777-1c2709d82d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ba04a-ba7d-4293-b6cb-ac442eb6572e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc4426-0846-4dfe-b02c-557bea0aeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacb649-8b23-4468-983c-d2511f71d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6c898-37b8-4f23-832f-4aaffba15280",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7d831-afe3-469c-a151-0141ee559bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_device = inputs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd4f1d-77db-48b2-933a-398c1e05f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e81402-cc80-4e7b-a0ab-adef78e4e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_mb = num_params * 4 / 1e6\n",
    "\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306d334-eb45-42e8-b828-9990047edd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del inputs\n",
    "del labels\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270b0e4-c985-4bc2-a0b6-a477faf5294b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f6adc-41c2-4a3c-b06f-f866cabf856c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
