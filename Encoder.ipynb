{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0dcf50-8ddd-4512-9eed-ece051f11c4a",
   "metadata": {},
   "source": [
    "## Creating an image decoder (Classifying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60011ab-f4fe-44f3-bdf8-1cc7e10ee5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceba79b7-89ef-4f44-9f49-77facd2a8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d383585a-d0d0-473e-9d02-8469c0d043cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "folder_path = 'video_frames/'\n",
    "\n",
    "image_paths = os.listdir(folder_path)\n",
    "image_paths = [folder_path + img_path for img_path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3464c74f-7451-471a-aa23-1bf28369d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = []\n",
    "for img in image_paths:\n",
    "    y.append( cv2.imread(img, cv2.IMREAD_GRAYSCALE) )\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.arange(y.shape[0])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "#X = nn.functional.one_hot(X, num_classes=y.shape[0]).float()\n",
    "y = torch.tensor(y).float()\n",
    "y = y/255 # ensures that values scale between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16aa2f43-4fb5-47ad-b8c0-f997b73d58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 6572\n",
      "Output dimensions: torch.Size([180, 240])\n"
     ]
    }
   ],
   "source": [
    "num_classes = y.shape[0]\n",
    "output_dimensions = y[0].shape\n",
    "print(f\"Num classes: {num_classes}\")\n",
    "print(f\"Output dimensions: {output_dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62202ba-d225-427e-8019-f7b7c71ae760",
   "metadata": {},
   "source": [
    "### Fully connected NN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a55bbf-889e-4a07-bf0e-cb6625f2a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # FC layer for feature mapping\n",
    "        self.fc = nn.Linear(num_classes, latent_dim * 3 * 4)\n",
    "\n",
    "        # Deconvolution layers\n",
    "        self.deconv_block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 100, \n",
    "                               kernel_size=5, stride=5, padding=0),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.deconv_block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 100, \n",
    "                               kernel_size=3, stride=3, padding=0),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.deconv_block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 100, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.deconv_block4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 100,\n",
    "                               kernel_size=2, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.deconv_block5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 1,\n",
    "                               kernel_size=2, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution\n",
    "        x = nn.functional.one_hot(x, num_classes=self.num_classes).float()\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.latent_dim, 3, 4)\n",
    "        \n",
    "        x1 = self.deconv_block1(x)\n",
    "        x2 = self.deconv_block2(x1)\n",
    "        x2 = x2 + torch.sum(x)/12  # skip connection\n",
    "        \n",
    "        x3 = self.deconv_block3(x2)\n",
    "        x4 = self.deconv_block4(x3)\n",
    "        x4 = x4 + torch.sum(x)/12 # skip connection\n",
    "        \n",
    "        x5 = self.deconv_block5(x4)\n",
    "        output = x5.view(-1, 180, 240)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6594f2f-4e41-44cb-9819-5a13d21931ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_Decoder(\n",
      "  (fc): Linear(in_features=6572, out_features=3072, bias=True)\n",
      "  (deconv_block1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 100, kernel_size=(5, 5), stride=(5, 5))\n",
      "    (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (deconv_block2): Sequential(\n",
      "    (0): ConvTranspose2d(100, 100, kernel_size=(3, 3), stride=(3, 3))\n",
      "    (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (deconv_block3): Sequential(\n",
      "    (0): ConvTranspose2d(100, 100, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (deconv_block4): Sequential(\n",
      "    (0): ConvTranspose2d(100, 100, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (deconv_block5): Sequential(\n",
      "    (0): ConvTranspose2d(100, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model = Conv_Decoder()\n",
    "model = torch.load('models/Conv_Decoder.pkl', weights_only=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65c31211-1f6d-4cbf-b6a1-70ff9bddd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "decay_1 = 0.9 # Decay of moving average of gradient\n",
    "decay_2 = 0.98 # Decay of moving average of squared gradient\n",
    "\n",
    "lr = 0.000001\n",
    "weight_decay = 0.0\n",
    "\n",
    "lr_decay_rate = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db1ae32d-dcf0-43b0-91ac-34042f72658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, Optimizer and Scheduler\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              betas = (decay_1, decay_2),\n",
    "                              lr=lr, \n",
    "                              weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3dd411-d6ab-49d4-a62f-454da8b4427a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96694f66-e22f-43e4-93ff-5e1f1110db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "inputs_device = X.device\n",
    "labels_device = y.device\n",
    "\n",
    "print(model_device)\n",
    "print(inputs_device)\n",
    "print(labels_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0217652b-d970-4482-93bd-52ef5fd9055b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a06b5b2e-bc3f-4889-9d60-334419b2e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 240])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(0, dtype=torch.long).to(device))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4896c69e-1108-4355-8cb7-d8e1c6cdb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(TensorDataset(X, y), batch_size=200, shuffle=True) # both train and test (deliberate overfit)\n",
    "\n",
    "num_epochs = 1000\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eda7b4-298b-4b4a-a176-048147ce6964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 0.014574\n",
      "Epoch 2/1000, Loss: 0.014568\n",
      "Epoch 3/1000, Loss: 0.014433\n",
      "Epoch 4/1000, Loss: 0.014544\n",
      "Epoch 5/1000, Loss: 0.014429\n",
      "Epoch 6/1000, Loss: 0.014601\n",
      "Epoch 7/1000, Loss: 0.014424\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "loss_values = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train() # Set to train mode\n",
    "    for inputs, labels in dataloader:\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed-forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add loss\n",
    "        batch_loss = loss.item()\n",
    "        running_loss += batch_loss\n",
    "        loss_values.append(batch_loss)\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # Update the learning rate at the end of each epoch\n",
    "    scheduler.step()\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab52e38-ddbf-4140-847f-dfe91a886a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d1bbb-a7eb-420d-b0ba-b3bceba60a18",
   "metadata": {},
   "source": [
    "### Model is too small to fully learn the images. I will freeze current weights and add another layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69b5a6-9470-4229-8c91-84542cc1a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcab43-7136-4cc9-8682-85faaac32f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Freezing params\n",
    "for module in model.deconv_block:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600e797-223f-4bfa-811d-b1875abea1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Updating model design\n",
    "new_deconv_block = nn.Sequential(\n",
    "    model.deconv_block[0],  # First ConvTranspose2d layer\n",
    "    model.deconv_block[1],  # ReLU\n",
    "    model.deconv_block[2],  # Second ConvTranspose2d layer\n",
    "    model.deconv_block[3],  # ReLU\n",
    "    # Add the new layer in the middle (e.g., ConvTranspose2d)\n",
    "    nn.ConvTranspose2d(100, 100, kernel_size=1, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    model.deconv_block[4],  # Third ConvTranspose2d layer\n",
    "    model.deconv_block[5],  # ReLU\n",
    "    model.deconv_block[6],  # Fourth ConvTranspose2d layer\n",
    "    model.deconv_block[7]   # Sigmoid\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bae1f1-ab05-491a-8777-1c2709d82d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deconv_block = new_deconv_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ba04a-ba7d-4293-b6cb-ac442eb6572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc4426-0846-4dfe-b02c-557bea0aeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.deconv_block[4].parameters():  # Unfreezing newly added layer\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacb649-8b23-4468-983c-d2511f71d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6c898-37b8-4f23-832f-4aaffba15280",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7d831-afe3-469c-a151-0141ee559bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_device = inputs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd4f1d-77db-48b2-933a-398c1e05f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e81402-cc80-4e7b-a0ab-adef78e4e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_mb = num_params * 4 / 1e6\n",
    "\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306d334-eb45-42e8-b828-9990047edd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del inputs\n",
    "del labels\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "952f6adc-41c2-4a3c-b06f-f866cabf856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/Conv_Decoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fa3fd-b792-49bb-9945-c69add295494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
