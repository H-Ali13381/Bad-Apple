{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0dcf50-8ddd-4512-9eed-ece051f11c4a",
   "metadata": {},
   "source": [
    "## Creating an image decoder (Classifying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60011ab-f4fe-44f3-bdf8-1cc7e10ee5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceba79b7-89ef-4f44-9f49-77facd2a8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d383585a-d0d0-473e-9d02-8469c0d043cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "folder_path = 'video_frames/'\n",
    "\n",
    "image_paths = os.listdir(folder_path)\n",
    "image_paths = [folder_path + img_path for img_path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3464c74f-7451-471a-aa23-1bf28369d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = []\n",
    "for img in image_paths:\n",
    "    y.append( cv2.imread(img, cv2.IMREAD_GRAYSCALE) )\n",
    "\n",
    "y = np.array(y)\n",
    "X = np.arange(y.shape[0])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "#X = nn.functional.one_hot(X, num_classes=y.shape[0]).float()\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16aa2f43-4fb5-47ad-b8c0-f997b73d58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 6572\n",
      "Output dimensions: torch.Size([180, 240])\n"
     ]
    }
   ],
   "source": [
    "num_classes = y.shape[0]\n",
    "output_dimensions = y[0].shape\n",
    "print(f\"Num classes: {num_classes}\")\n",
    "print(f\"Output dimensions: {output_dimensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62202ba-d225-427e-8019-f7b7c71ae760",
   "metadata": {},
   "source": [
    "### Fully connected NN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d4fcef8-6e60-4073-b134-af3b335f9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # FC layer for feature mapping\n",
    "        self.fc = nn.Linear(num_classes, latent_dim * 3 * 4)\n",
    "\n",
    "        # Deconvolution layers\n",
    "        self.conv_1 = nn.ConvTranspose2d(latent_dim, 10,\n",
    "                           kernel_size=5, stride=5, padding=0)\n",
    "        self.conv_2 = nn.ConvTranspose2d(10, 10, \n",
    "                           kernel_size=3, stride=3, padding=0)\n",
    "        self.conv_3 = nn.ConvTranspose2d(10, 10,\n",
    "                           kernel_size=2, stride=2, padding=0)\n",
    "        self.conv_4 = nn.ConvTranspose2d(10, num_classes,\n",
    "                           kernel_size=2, stride=2, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.one_hot(x, num_classes=self.num_classes).float()\n",
    "        x = self.fc(x)\n",
    "        x = x.view(self.latent_dim, 3, 4)\n",
    "        print(f\"Shape after conv0: {x.shape}\")\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Shape after conv1: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Shape after conv2: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Shape after conv3: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        print(f\"Shape after conv4: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8a55bbf-889e-4a07-bf0e-cb6625f2a854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Conv_Decoder(nn.Module):\\n    def __init__(self, latent_dim=1, num_classes=num_classes):\\n        super().__init__()\\n        self.latent_dim = latent_dim\\n        self.num_classes = num_classes\\n\\n        # FC layer for feature mapping\\n        self.fc = nn.Linear(num_classes, latent_dim * 3 * 4)\\n\\n        # Deconvolution layers\\n        self.deconv_block = nn.Sequential(\\n            nn.ConvTranspose2d(latent_dim, 10, \\n                               kernel_size=5, stride=3, padding=1),\\n            nn.ReLU(),\\n            nn.ConvTranspose2d(10, 10, \\n                               kernel_size=5, stride=3, padding=1),\\n            nn.ReLU(),\\n            nn.ConvTranspose2d(10, 10,\\n                               kernel_size=5, stride=3, padding=1),\\n            nn.ReLU(),\\n            nn.ConvTranspose2d(10, 1,\\n                               kernel_size=1, stride=2, padding=0),\\n            nn.Sigmoid()\\n        )\\n\\n    def forward(self, x):\\n        x = nn.functional.one_hot(x, num_classes=self.num_classes).float()\\n        x = self.fc(x)\\n        x = x.view(-1, self.latent_dim, 3, 4)\\n        x = self.deconv_block(x)\\n        return x\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class Conv_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1, num_classes=num_classes):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # FC layer for feature mapping\n",
    "        self.fc = nn.Linear(num_classes, latent_dim * 3 * 4)\n",
    "\n",
    "        # Deconvolution layers\n",
    "        self.deconv_block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 10, \n",
    "                               kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(10, 10, \n",
    "                               kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(10, 10,\n",
    "                               kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(10, 1,\n",
    "                               kernel_size=1, stride=2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.one_hot(x, num_classes=self.num_classes).float()\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.latent_dim, 3, 4)\n",
    "        x = self.deconv_block(x)\n",
    "        return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "366c1336-07e7-4c46-a043-833e7774a63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX1 = nn.functional.one_hot(X[5], num_classes=num_classes).float()\\nlinear_ = nn.Linear(num_classes, latent_dim * 3 * 4)\\nX2 = linear_(X1)\\nX3 = X2.view(-1, latent_dim, 3, 4)\\nX3.shape\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X1 = nn.functional.one_hot(X[5], num_classes=num_classes).float()\n",
    "linear_ = nn.Linear(num_classes, latent_dim * 3 * 4)\n",
    "X2 = linear_(X1)\n",
    "X3 = X2.view(-1, latent_dim, 3, 4)\n",
    "X3.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6594f2f-4e41-44cb-9819-5a13d21931ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_Decoder(\n",
      "  (fc): Linear(in_features=6572, out_features=12, bias=True)\n",
      "  (conv_1): ConvTranspose2d(1, 10, kernel_size=(5, 5), stride=(5, 5))\n",
      "  (conv_2): ConvTranspose2d(10, 10, kernel_size=(3, 3), stride=(3, 3))\n",
      "  (conv_3): ConvTranspose2d(10, 10, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv_4): ConvTranspose2d(10, 6572, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Conv_Decoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a06b5b2e-bc3f-4889-9d60-334419b2e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after conv0: torch.Size([1, 3, 4])\n",
      "Shape after conv1: torch.Size([10, 15, 20])\n",
      "Shape after conv2: torch.Size([10, 45, 60])\n",
      "Shape after conv3: torch.Size([10, 90, 120])\n",
      "Shape after conv4: torch.Size([6572, 180, 240])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4986, 0.4988, 0.4985,  ..., 0.4988, 0.4985, 0.4988],\n",
       "         [0.4984, 0.4984, 0.4984,  ..., 0.4984, 0.4984, 0.4984],\n",
       "         [0.4985, 0.4988, 0.4985,  ..., 0.4988, 0.4985, 0.4988],\n",
       "         ...,\n",
       "         [0.4984, 0.4984, 0.4984,  ..., 0.4984, 0.4984, 0.4984],\n",
       "         [0.4985, 0.4988, 0.4985,  ..., 0.4988, 0.4985, 0.4988],\n",
       "         [0.4984, 0.4984, 0.4984,  ..., 0.4984, 0.4984, 0.4984]],\n",
       "\n",
       "        [[0.5006, 0.5005, 0.5006,  ..., 0.5005, 0.5006, 0.5005],\n",
       "         [0.5004, 0.5006, 0.5004,  ..., 0.5006, 0.5004, 0.5006],\n",
       "         [0.5006, 0.5005, 0.5006,  ..., 0.5005, 0.5006, 0.5005],\n",
       "         ...,\n",
       "         [0.5004, 0.5006, 0.5004,  ..., 0.5006, 0.5004, 0.5006],\n",
       "         [0.5006, 0.5005, 0.5006,  ..., 0.5005, 0.5006, 0.5005],\n",
       "         [0.5004, 0.5006, 0.5004,  ..., 0.5006, 0.5004, 0.5006]],\n",
       "\n",
       "        [[0.4990, 0.4988, 0.4990,  ..., 0.4988, 0.4990, 0.4988],\n",
       "         [0.4990, 0.4987, 0.4990,  ..., 0.4987, 0.4990, 0.4987],\n",
       "         [0.4990, 0.4988, 0.4990,  ..., 0.4988, 0.4990, 0.4988],\n",
       "         ...,\n",
       "         [0.4990, 0.4987, 0.4990,  ..., 0.4987, 0.4990, 0.4987],\n",
       "         [0.4990, 0.4988, 0.4990,  ..., 0.4988, 0.4990, 0.4988],\n",
       "         [0.4990, 0.4987, 0.4990,  ..., 0.4987, 0.4990, 0.4987]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5001, 0.5002, 0.5001,  ..., 0.5002, 0.5001, 0.5002],\n",
       "         [0.5001, 0.5000, 0.5001,  ..., 0.5000, 0.5001, 0.5000],\n",
       "         [0.5001, 0.5002, 0.5001,  ..., 0.5002, 0.5001, 0.5002],\n",
       "         ...,\n",
       "         [0.5001, 0.5000, 0.5001,  ..., 0.5000, 0.5001, 0.5000],\n",
       "         [0.5001, 0.5002, 0.5001,  ..., 0.5002, 0.5001, 0.5002],\n",
       "         [0.5001, 0.5000, 0.5002,  ..., 0.5000, 0.5001, 0.5000]],\n",
       "\n",
       "        [[0.4985, 0.4985, 0.4985,  ..., 0.4985, 0.4985, 0.4985],\n",
       "         [0.4987, 0.4984, 0.4987,  ..., 0.4984, 0.4987, 0.4985],\n",
       "         [0.4985, 0.4985, 0.4985,  ..., 0.4985, 0.4985, 0.4985],\n",
       "         ...,\n",
       "         [0.4987, 0.4984, 0.4987,  ..., 0.4984, 0.4987, 0.4985],\n",
       "         [0.4985, 0.4985, 0.4985,  ..., 0.4985, 0.4985, 0.4985],\n",
       "         [0.4987, 0.4985, 0.4987,  ..., 0.4985, 0.4987, 0.4985]],\n",
       "\n",
       "        [[0.5013, 0.5012, 0.5013,  ..., 0.5012, 0.5013, 0.5012],\n",
       "         [0.5015, 0.5016, 0.5015,  ..., 0.5016, 0.5015, 0.5016],\n",
       "         [0.5014, 0.5012, 0.5013,  ..., 0.5012, 0.5013, 0.5012],\n",
       "         ...,\n",
       "         [0.5015, 0.5016, 0.5015,  ..., 0.5016, 0.5015, 0.5016],\n",
       "         [0.5014, 0.5012, 0.5013,  ..., 0.5012, 0.5013, 0.5012],\n",
       "         [0.5015, 0.5016, 0.5015,  ..., 0.5016, 0.5015, 0.5016]]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.tensor(0, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65c31211-1f6d-4cbf-b6a1-70ff9bddd184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Hyperparameters\\ndecay_1 = 0.9 # Decay of moving average of gradient\\ndecay_2 = 0.99 # Decay of moving average of squared gradient\\n\\nlr = 0.0001\\nweight_decay = 0.00004\\n\\nlr_decay_rate = 0.98\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Hyperparameters\n",
    "decay_1 = 0.9 # Decay of moving average of gradient\n",
    "decay_2 = 0.99 # Decay of moving average of squared gradient\n",
    "\n",
    "lr = 0.0001\n",
    "weight_decay = 0.00004\n",
    "\n",
    "lr_decay_rate = 0.98\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1ae32d-dcf0-43b0-91ac-34042f72658a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Loss function, Optimizer and Scheduler\\ncriterion = torch.nn.MSELoss()\\noptimizer = torch.optim.AdamW(model.parameters(), \\n                              betas = (decay_1, decay_2),\\n                              lr=lr, \\n                              weight_decay=weight_decay)\\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay_rate)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Loss function, Optimizer and Scheduler\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              betas = (decay_1, decay_2),\n",
    "                              lr=lr, \n",
    "                              weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay_rate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68e010df-8f97-41ae-b5e8-3cd601c70224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, Optimizer and Scheduler\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3dd411-d6ab-49d4-a62f-454da8b4427a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96694f66-e22f-43e4-93ff-5e1f1110db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "inputs_device = X.device\n",
    "labels_device = y.device\n",
    "\n",
    "print(model_device)\n",
    "print(inputs_device)\n",
    "print(labels_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4896c69e-1108-4355-8cb7-d8e1c6cdb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(TensorDataset(X, y), batch_size=1, shuffle=True) # both train and test (deliberate overfit)\n",
    "\n",
    "num_epochs = 10\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61eda7b4-298b-4b4a-a176-048147ce6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neopa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1, 180, 240])) that is different to the input size (torch.Size([1, 6572, 161, 215])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (215) must match the size of tensor b (240) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Feed-forward\u001b[39;00m\n\u001b[0;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\functional.py:3884\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[0;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3882\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3884\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (215) must match the size of tensor b (240) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train() # Set to train mode\n",
    "    for inputs, labels in dataloader:\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed-forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add loss\n",
    "        batch_loss = loss.item()\n",
    "        running_loss += batch_loss\n",
    "        loss_values.append(batch_loss)\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # Training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation accuracy\n",
    "    model.eval()  # Eval mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): #No gradient\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    # Update the learning rate at the end of each epoch\n",
    "    scheduler.step()\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "        print(f\"Train accuracy: {train_acc}, Validation accuracy: {val_acc}\")\n",
    "\n",
    "    if early_stopping==True and val_acc > 0.999:\n",
    "        print(\"Achieved acceptable accuracy. Stopping early.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacb649-8b23-4468-983c-d2511f71d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6c898-37b8-4f23-832f-4aaffba15280",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7d831-afe3-469c-a151-0141ee559bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_device = inputs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd4f1d-77db-48b2-933a-398c1e05f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e81402-cc80-4e7b-a0ab-adef78e4e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_mb = num_params * 4 / 1e6\n",
    "\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3306d334-eb45-42e8-b828-9990047edd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del inputs\n",
    "del labels\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270b0e4-c985-4bc2-a0b6-a477faf5294b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f6adc-41c2-4a3c-b06f-f866cabf856c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
